import os
import random
from enum import Enum
import numpy as np

# ---------------------------
# this code complitly generated by ai , its base on what i wrote in maze.py , but updated so it work with markov decision process
# ---------------------------


class ObjectsInGame(Enum):
    WALL = -1
    SPACE = 0
    GOAL = 1


class Maze:
    def __init__(self, game_size, max_steps=None, extra_paths=5, fixed_maze_episodes=0):
        self.play_ground_size = game_size + 1
        self.player_location = (1, 1)
        self.goal_location = (game_size - 1, game_size - 1)
        self.play_ground = {}
        self.wall_range = self.play_ground_size - 1
        self.max_steps = max_steps if max_steps is not None else game_size * 10
        self.current_step = 0
        self.actions = [0, 1, 2, 3]  # 0: up, 1: down, 2: left, 3: right
        self.action_map = {
            0: (-1, 0),  # up
            1: (1, 0),  # down
            2: (0, -1),  # left
            3: (0, 1),  # right
        }
        self.extra_paths = extra_paths
        self.fixed_maze_episodes = fixed_maze_episodes
        self.episode_count = 0
        self.fixed_maze = None
        self.prev_distance = None

    def reset(self):
        """Reset the environment to the initial state."""
        self.player_location = (1, 1)
        self.current_step = 0
        self.prev_distance = self._manhattan_distance(
            self.player_location, self.goal_location
        )

        if (
            self.fixed_maze_episodes == 0
            or self.episode_count % self.fixed_maze_episodes != 0
        ):
            self.play_ground = {}
            self.generate_playGround()
            self.add_extra_paths(extra_count=self.extra_paths)
            if self.fixed_maze_episodes > 0:
                self.fixed_maze = self.play_ground.copy()
        else:
            self.play_ground = self.fixed_maze.copy()

        self.episode_count += 1
        return self.get_state()

    def get_state(self):
        """Return the current state as a batched numpy array."""
        return np.array([self.player_location], dtype=np.float32)  # Shape: (1, 2)

    def get_actions(self, only_valid=False):
        """Return the list of possible actions. If only_valid=True, return only valid actions."""
        if not only_valid:
            return self.actions
        valid_actions = []
        for action in self.actions:
            dx, dy = self.action_map[action]
            new_x, new_y = self.player_location[0] + dx, self.player_location[1] + dy
            if (
                0 < new_x < self.wall_range
                and 0 < new_y < self.wall_range
                and self.play_ground.get((new_x, new_y), ObjectsInGame.WALL)
                != ObjectsInGame.WALL
            ):
                valid_actions.append(action)
        return (
            valid_actions if valid_actions else self.actions
        )  # Fallback to all actions if none valid

    def step(self, action):
        """Take an action and return (next_state, reward, done, info)."""
        self.current_step += 1
        dx, dy = self.action_map[action]
        new_x, new_y = self.player_location[0] + dx, self.player_location[1] + dy
        reward = -1  # Default step penalty
        done = False
        info = {}

        # Check if the move is valid
        if 0 < new_x < self.wall_range and 0 < new_y < self.wall_range:
            if (
                self.play_ground.get((new_x, new_y), ObjectsInGame.WALL)
                != ObjectsInGame.WALL
            ):
                self.player_location = (new_x, new_y)  # Update player position
                if self.play_ground[(new_x, new_y)] == ObjectsInGame.GOAL:
                    reward = 10  # Reward for reaching the goal
                    done = True
                else:
                    # Shaping reward based on Manhattan distance reduction
                    curr_distance = self._manhattan_distance(
                        self.player_location, self.goal_location
                    )
                    reward += (
                        self.prev_distance - curr_distance
                    ) * 0.5  # Bonus for getting closer
                    self.prev_distance = curr_distance
            else:
                reward = -5  # Penalty for hitting a wall
        else:
            reward = -5  # Penalty for hitting the boundary

        # Check if episode is done due to max steps
        if self.current_step >= self.max_steps:
            done = True

        next_state = self.get_state()
        return next_state, reward, done, info

    def _manhattan_distance(self, pos1, pos2):
        """Calculate Manhattan distance between two positions."""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])

    def generate_playGround(self):
        """Generate the maze layout."""
        for row in range(self.play_ground_size):
            for col in range(self.play_ground_size):
                self.play_ground[(row, col)] = ObjectsInGame.WALL
        self._carve_the_path(self.player_location[0], self.player_location[1])
        self.play_ground[self.goal_location] = ObjectsInGame.GOAL

    def generate_playGround_pattern(self):
        """Generate a visual representation of the maze."""
        pattern = []
        for row in range(self.play_ground_size):
            pattern_row = []
            for col in range(self.play_ground_size):
                pos = (row, col)
                if (
                    pos in self.play_ground
                    and self.play_ground[pos] == ObjectsInGame.WALL
                ):
                    pattern_row.append("#")
                elif pos == self.goal_location:
                    pattern_row.append("G")
                elif pos == self.player_location:
                    pattern_row.append("P")
                else:
                    pattern_row.append(" ")
            pattern.append(pattern_row)
        return pattern

    def _carve_the_path(self, x, y):
        """Recursively carve paths in the maze."""
        self.play_ground[(x, y)] = ObjectsInGame.SPACE
        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]
        random.shuffle(directions)
        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            if 0 < nx < self.wall_range and 0 < ny < self.wall_range:
                if (
                    self.play_ground.get((nx, ny), ObjectsInGame.WALL)
                    == ObjectsInGame.WALL
                ):
                    mid_x = (x + nx) // 2
                    mid_y = (y + ny) // 2
                    self.play_ground[(mid_x, mid_y)] = ObjectsInGame.SPACE
                    self._carve_the_path(nx, ny)

    def add_extra_paths(self, extra_count=10):
        """Add extra paths to the maze."""
        attempts = 0
        added = 0
        while added < extra_count and attempts < extra_count * 5:
            x = random.randint(1, self.wall_range - 1)
            y = random.randint(1, self.wall_range - 1)
            if x % 2 == 1 or y % 2 == 1:  # Only consider wall positions
                neighbors = [
                    ((x - 1, y), (x + 1, y)),
                    ((x, y - 1), (x, y + 1)),
                ]
                for a, b in neighbors:
                    if (
                        a in self.play_ground
                        and b in self.play_ground
                        and self.play_ground.get(a, ObjectsInGame.WALL)
                        == ObjectsInGame.SPACE
                        and self.play_ground.get(b, ObjectsInGame.WALL)
                        == ObjectsInGame.SPACE
                    ):
                        self.play_ground[(x, y)] = ObjectsInGame.SPACE
                        added += 1
                        break
            attempts += 1

    def clear_screen(self):
        """Clear the console screen."""
        os.system("cls" if os.name == "nt" else "clear")

    def render(self):
        """Render the maze (for visualization during training)."""
        self.clear_screen()
        maze_pattern = self.generate_playGround_pattern()
        for row in maze_pattern:
            print(" ".join(row))
