import os
import random
from enum import Enum
import numpy as np
# ---------------------------
# this code complitly generated by ai , its base on what i wrote in maze.py , but updated so it work with markov decision process
# ---------------------------


class objects_in_game(Enum):
    wall = -1
    space = 0
    goal = 1


class maze:
    def __init__(self, game_size, max_steps=100) -> None:
        # game_size + walls
        self.play_ground_size = game_size + 1
        self.player_location = (1, 1)
        self.goal_location = (game_size - 1, game_size - 1)
        self.play_ground = {}
        self.wall_range = self.play_ground_size - 1
        self.max_steps = max_steps  # Maximum steps per episode
        self.current_step = 0
        self.actions = [0, 1, 2, 3]  # 0: up, 1: down, 2: left, 3: right
        self.action_map = {
            0: (-1, 0),  # up
            1: (1, 0),  # down
            2: (0, -1),  # left
            3: (0, 1),  # right
        }

    def reset(self):
        """Reset the environment to the initial state."""
        self.player_location = (1, 1)
        self.current_step = 0
        self.play_ground = {}
        self.generate_playGround()
        self.add_extra_paths(extra_count=20)
        return self.get_state()

    def get_state(self):
        """Return the current state (player's position as a tuple or array)."""
        return np.array(self.player_location, dtype=np.float32)

    def get_actions(self):
        """Return the list of possible actions."""
        return self.actions

    def step(self, action):
        """Take an action and return (next_state, reward, done, info)."""
        self.current_step += 1
        dx, dy = self.action_map[action]
        new_x, new_y = self.player_location[0] + dx, self.player_location[1] + dy
        reward = -1  # Default step penalty
        done = False
        info = {}

        # Check if the move is valid
        if 0 < new_x < self.wall_range and 0 < new_y < self.wall_range:
            if self.play_ground[(new_x, new_y)] != objects_in_game.wall:
                self.player_location = (new_x, new_y)  # Update player position
                if self.play_ground[(new_x, new_y)] == objects_in_game.goal:
                    reward = 10  # Reward for reaching the goal
                    done = True
            else:
                reward = -5  # Penalty for hitting a wall
        else:
            reward = -5  # Penalty for hitting the boundary

        # Check if episode is done due to max steps
        if self.current_step >= self.max_steps:
            done = True

        next_state = self.get_state()
        return next_state, reward, done, info

    def generate_playGround(self):
        """Generate the maze layout."""
        for row in range(self.play_ground_size):
            for col in range(self.play_ground_size):
                self.play_ground[(row, col)] = objects_in_game.wall
        self._carve_the_path(self.player_location[0], self.player_location[1])
        self.play_ground[self.goal_location] = objects_in_game.goal

    def generate_playGround_pattern(self):
        """Generate a visual representation of the maze."""
        pattern = []
        for row in range(self.play_ground_size):
            pattern_row = []
            for col in range(self.play_ground_size):
                pos = (row, col)
                if (
                    pos in self.play_ground
                    and self.play_ground[pos] == objects_in_game.wall
                ):
                    pattern_row.append("#")
                elif pos == self.goal_location:
                    pattern_row.append("G")
                elif pos == self.player_location:
                    pattern_row.append("P")
                else:
                    pattern_row.append(" ")
            pattern.append(pattern_row)
        return pattern

    def _carve_the_path(self, x, y):
        """Recursively carve paths in the maze."""
        self.play_ground[(x, y)] = objects_in_game.space
        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]
        random.shuffle(directions)
        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            if 0 < nx < self.wall_range and 0 < ny < self.wall_range:
                if self.play_ground.get((nx, ny)) == objects_in_game.wall:
                    mid_x = (x + nx) // 2
                    mid_y = (y + ny) // 2
                    self.play_ground[(mid_x, mid_y)] = objects_in_game.space
                    self._carve_the_path(nx, ny)

    def add_extra_paths(self, extra_count=10):
        """Add extra paths to the maze."""
        attempts = 0
        added = 0
        while added < extra_count and attempts < extra_count * 5:
            x = random.randint(1, self.wall_range - 1)
            y = random.randint(1, self.wall_range - 1)
            if x % 2 == 1 or y % 2 == 1:  # Only consider wall positions
                neighbors = [
                    ((x - 1, y), (x + 1, y)),
                    ((x, y - 1), (x, y + 1)),
                ]
                for a, b in neighbors:
                    if (
                        a in self.play_ground
                        and b in self.play_ground
                        and self.play_ground[a] == objects_in_game.space
                        and self.play_ground[b] == objects_in_game.space
                    ):
                        self.play_ground[(x, y)] = objects_in_game.space
                        added += 1
                        break
            attempts += 1

    def clear_screen(self):
        """Clear the console screen."""
        os.system("cls" if os.name == "nt" else "clear")

    def render(self):
        """Render the maze (for visualization during training)."""
        self.clear_screen()
        maze_pattern = self.generate_playGround_pattern()
        for row in maze_pattern:
            print(" ".join(row))


if __name__ == "__main__":
    env = maze(game_size=10)
    state = env.reset()
    env.render()
    done = False
    while not done:
        action = random.choice(env.get_actions())  # Random action for testing
        next_state, reward, done, info = env.step(action)
        env.render()
        print(f"State: {state}, Action: {action}, Reward: {reward}, Done: {done}")
        state = next_state
